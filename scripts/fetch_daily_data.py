"""
Aè‚¡æ¯æ—¥å¤ç›˜æ•°æ®è‡ªåŠ¨é‡‡é›†è„šæœ¬

ç”¨æ³•ï¼š
    python3 scripts/fetch_daily_data.py                          # é‡‡é›†ä»Šå¤©çš„æ•°æ®
    python3 scripts/fetch_daily_data.py 20260225                 # é‡‡é›†æŒ‡å®šæ—¥æœŸ
    python3 scripts/fetch_daily_data.py --range 20260210 20260225            # æ‰¹é‡é‡‡é›†æ—¥æœŸèŒƒå›´
    python3 scripts/fetch_daily_data.py --days 5                             # é‡‡é›†æœ€è¿‘5ä¸ªäº¤æ˜“æ—¥
    python3 scripts/fetch_daily_data.py --range 20260210 20260225 --summary  # æ‰¹é‡é‡‡é›†å¹¶ç”Ÿæˆæ±‡æ€»
    python3 scripts/fetch_daily_data.py --range 20260210 20260225 --force    # å¼ºåˆ¶é‡æ–°é‡‡é›†ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
    python3 scripts/fetch_daily_data.py --print-only                         # ä»…æ‰“å°ï¼Œä¸ç”Ÿæˆæ–‡ä»¶

æ•°æ®æºï¼šAKShareï¼ˆæ–°æµª + ä¸œæ–¹è´¢å¯Œï¼‰
å·²éªŒè¯å¯ç”¨æ¥å£ï¼ˆ2026-02ï¼‰ï¼š
  - stock_zh_index_spot_sina    æŒ‡æ•°å®æ—¶ï¼ˆæ–°æµªæºï¼‰
  - stock_zh_index_daily        æŒ‡æ•°å†å²æ—¥çº¿
  - stock_zh_a_spot             å…¨Aå®æ—¶è¡Œæƒ…ï¼ˆæ–°æµªæºï¼‰
  - stock_zt_pool_em            æ¶¨åœè‚¡æ± ï¼ˆä¸œè´¢æ¶¨åœä¸“é¢˜ï¼‰
  - stock_zt_pool_zbgc_em       ç‚¸æ¿è‚¡æ± 
  - stock_zt_pool_dtgc_em       è·Œåœè‚¡æ± 
  - stock_zt_pool_previous_em   æ˜¨æ—¥æ¶¨åœè‚¡æ± 
"""

from __future__ import annotations

import argparse
import json
import warnings
from collections import defaultdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

import akshare as ak
import pandas as pd

warnings.filterwarnings("ignore")

DATA_DIR = Path(__file__).resolve().parent.parent / "data"


# ---------------------------------------------------------------------------
# äº¤æ˜“æ—¥å†
# ---------------------------------------------------------------------------

try:
    import exchange_calendars as xcals
    _XSHG = xcals.get_calendar("XSHG")
    _HAS_XCALS = True
except ImportError:
    _HAS_XCALS = False


def get_trading_days(start: str, end: str) -> list[str]:
    """è¿”å› [start, end] èŒƒå›´å†…çš„ A è‚¡äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆYYYYMMDD å­—ç¬¦ä¸²ï¼‰ã€‚"""
    s = pd.Timestamp(start)
    e = pd.Timestamp(end)
    if _HAS_XCALS:
        try:
            sessions = _XSHG.sessions_in_range(s, e)
            return [d.strftime("%Y%m%d") for d in sessions]
        except Exception:
            pass
    if not _HAS_XCALS:
        print("  âš  exchange_calendars æœªå®‰è£…ï¼Œä»…æŒ‰å·¥ä½œæ—¥è¿‡æ»¤ï¼ˆèŠ‚å‡æ—¥å¯èƒ½è¯¯åˆ¤ï¼‰")
    else:
        print("  âš  æ—¥æœŸè¶…å‡º exchange_calendars èŒƒå›´ï¼Œå›é€€ä¸ºå·¥ä½œæ—¥è¿‡æ»¤")
    days = pd.bdate_range(s, e)
    return [d.strftime("%Y%m%d") for d in days]


def get_recent_trading_days(n: int) -> list[str]:
    """è¿”å›è·ä»Šæœ€è¿‘çš„ N ä¸ªäº¤æ˜“æ—¥ã€‚"""
    today = pd.Timestamp(datetime.now().date())
    lookback = today - pd.Timedelta(days=max(n * 3, 60))
    all_days = get_trading_days(lookback.strftime("%Y%m%d"), today.strftime("%Y%m%d"))
    return all_days[-n:]


# ---------------------------------------------------------------------------
# æƒ…ç»ªè¯„åˆ†æ˜ å°„
# ---------------------------------------------------------------------------

def score_zt_count(n: int) -> float:
    if n < 20: return max(1, n / 10)
    if n < 40: return 3 + (n - 20) / 20 * 2
    if n < 70: return 5 + (n - 40) / 30 * 2
    if n < 100: return 7 + (n - 70) / 30 * 2
    return min(10, 9 + (n - 100) / 50)

def score_seal_rate(rate: float) -> float:
    if rate < 40: return max(1, rate / 40 * 3)
    if rate < 55: return 4 + (rate - 40) / 15
    if rate < 70: return 6 + (rate - 55) / 15
    if rate < 85: return 8 + (rate - 70) / 15
    return min(10, 9 + (rate - 85) / 15)

def score_premium(pct: float) -> float:
    if pct < -5: return 1
    if pct < -2: return 1 + (pct + 5) / 3 * 2
    if pct < 0: return 4 + (pct + 2) / 2
    if pct < 3: return 6 + pct / 3
    if pct < 6: return 8 + (pct - 3) / 3
    return min(10, 9 + (pct - 6) / 4)

def score_max_streak(n: int) -> float:
    mapping = {0: 1, 1: 2, 2: 3, 3: 5, 4: 7, 5: 7, 6: 8, 7: 8}
    if n in mapping: return mapping[n]
    return min(10, 9 + (n - 7) / 3)

def score_rise_fall_ratio(ratio: float) -> float:
    if ratio < 0.3: return 1
    if ratio < 0.6: return 2 + (ratio - 0.3) / 0.3
    if ratio < 1: return 4 + (ratio - 0.6) / 0.4 * 2
    if ratio < 2: return 6 + (ratio - 1)
    if ratio < 3: return 8 + (ratio - 2)
    return min(10, 9 + (ratio - 3) / 2)

def score_dt_count(n: int) -> float:
    if n == 0: return 10
    if n <= 5: return 8 + (5 - n) / 5
    if n <= 10: return 6 + (10 - n) / 5 * 2
    if n <= 20: return 4 + (20 - n) / 10 * 2
    if n <= 30: return 2 + (30 - n) / 10
    return 1

def calc_emotion_score(zt_count, seal_rate, premium, max_streak, rf_ratio, dt_count):
    scores = {
        "æ¶¨åœå®¶æ•°": score_zt_count(zt_count),
        "å°æ¿ç‡": score_seal_rate(seal_rate),
        "æ˜¨æ¶¨åœæº¢ä»·": score_premium(premium),
        "è¿æ¿é«˜åº¦": score_max_streak(max_streak),
        "æ¶¨è·Œæ¯”": score_rise_fall_ratio(rf_ratio),
        "è·ŒåœåæŒ‡": score_dt_count(dt_count),
    }
    weights = {"æ¶¨åœå®¶æ•°": 0.15, "å°æ¿ç‡": 0.15, "æ˜¨æ¶¨åœæº¢ä»·": 0.25,
               "è¿æ¿é«˜åº¦": 0.15, "æ¶¨è·Œæ¯”": 0.15, "è·ŒåœåæŒ‡": 0.15}
    total = sum(scores[k] * weights[k] for k in scores)
    return round(total, 2), {k: round(v, 1) for k, v in scores.items()}

def emotion_stage(score: float) -> str:
    if score < 3: return "å†°ç‚¹æœŸ"
    if score < 5: return "é€€æ½®æœŸ"
    if score < 7: return "å›æš–æœŸ"
    if score < 9: return "é«˜æ½®æœŸ"
    return "äº¢å¥‹æœŸ"

def emotion_stage_full(score: float) -> str:
    if score < 3: return "å†°ç‚¹æœŸï¼ˆ1-3åˆ†ï¼‰â€”â€” æåº¦ä½è¿·ï¼Œç©ºä»“è§‚æœ›"
    if score < 5: return "é€€æ½®/ä¿®å¤æœŸï¼ˆ3-5åˆ†ï¼‰â€”â€” äºé’±æ•ˆåº”ä¸ºä¸»ï¼Œæ§ä»“å…³æ³¨è½¬æŠ˜"
    if score < 7: return "å›æš–/ä¸Šå‡æœŸï¼ˆ5-7åˆ†ï¼‰â€”â€” èµšé’±æ•ˆåº”å›å½’ï¼Œè·Ÿéšé¾™å¤´"
    if score < 9: return "é«˜æ½®æœŸï¼ˆ7-9åˆ†ï¼‰â€”â€” èµšé’±æ•ˆåº”å¼ºçƒˆï¼Œæ³¨æ„è§é¡¶ä¿¡å·"
    return "æåº¦äº¢å¥‹ï¼ˆ9-10åˆ†ï¼‰â€”â€” è­¦æƒ•é€€æ½®ï¼Œå¼€å§‹é˜²å®ˆ"


# ---------------------------------------------------------------------------
# æ•°æ®é‡‡é›†
# ---------------------------------------------------------------------------

def _safe_fetch(name, fn):
    try:
        return fn()
    except Exception as e:
        print(f"  âš  {name} è·å–å¤±è´¥: {e}")
        return pd.DataFrame()

def fetch_index_data() -> dict:
    """ä¸‰å¤§æŒ‡æ•°å®æ—¶æ•°æ®ï¼ˆæ–°æµªæºï¼‰â€”â€” ä»…å•æ—¥æ¨¡å¼ä½¿ç”¨"""
    df = ak.stock_zh_index_spot_sina()
    target = {"sh000001": "ä¸Šè¯æŒ‡æ•°", "sz399001": "æ·±è¯æˆæŒ‡", "sz399006": "åˆ›ä¸šæ¿æŒ‡"}
    result = {}
    for _, row in df.iterrows():
        code = str(row["ä»£ç "])
        if code in target:
            result[target[code]] = {
                "æ”¶ç›˜": round(float(row["æœ€æ–°ä»·"]), 2),
                "æ¶¨è·Œå¹…": round(float(row["æ¶¨è·Œå¹…"]), 2),
                "æˆäº¤é¢_äº¿": round(float(row["æˆäº¤é¢"]) / 1e8, 0),
            }
    return result

def fetch_index_hist(dates: list[str]) -> dict[str, dict]:
    """æ‰¹é‡è·å–æŒ‡æ•°å†å²æ•°æ®ï¼Œè¿”å› {date_str: {name: {close, pct, volume}}}"""
    result: dict[str, dict] = {}
    for code, name in [("sh000001", "ä¸Šè¯æŒ‡æ•°"), ("sz399001", "æ·±è¯æˆæŒ‡"), ("sz399006", "åˆ›ä¸šæ¿æŒ‡")]:
        try:
            df = ak.stock_zh_index_daily(symbol=code)
            df["date_str"] = pd.to_datetime(df["date"]).dt.strftime("%Y%m%d")
            df = df.sort_values("date").reset_index(drop=True)
            df["pct"] = df["close"].pct_change() * 100
            df_filtered = df[df["date_str"].isin(dates)]
            for _, row in df_filtered.iterrows():
                d = row["date_str"]
                if d not in result:
                    result[d] = {}
                result[d][name] = {
                    "æ”¶ç›˜": round(float(row["close"]), 2),
                    "æ¶¨è·Œå¹…": round(float(row["pct"]), 2) if pd.notna(row["pct"]) else 0.0,
                    "æˆäº¤é¢_äº¿": round(float(row["volume"]) / 1e8, 0) if "volume" in row and row["volume"] else 0,
                }
        except Exception as e:
            print(f"  âš  {name} å†å²æ•°æ®è·å–å¤±è´¥: {e}")
    return result

def fetch_a_spot() -> pd.DataFrame:
    """å…¨Aå®æ—¶è¡Œæƒ…ï¼ˆæ–°æµªæºï¼‰â€”â€” ä»…å•æ—¥æ¨¡å¼ä½¿ç”¨ã€‚è¿”å›æ¸…æ´—åçš„ DataFrameã€‚"""
    df = ak.stock_zh_a_spot()
    df["æ¶¨è·Œå¹…"] = pd.to_numeric(df["æ¶¨è·Œå¹…"], errors="coerce")
    df = df[~df["ä»£ç "].astype(str).str.startswith("bj")]
    df = df[~df["åç§°"].astype(str).str.contains("ST")]
    df = df[pd.notna(df["æ¶¨è·Œå¹…"])]
    df = df[pd.to_numeric(df["æˆäº¤é‡"], errors="coerce") > 0]
    return df

def calc_rise_fall_stats(df: pd.DataFrame) -> dict:
    """ä»å…¨Aè¡Œæƒ… DataFrame è®¡ç®—æ¶¨è·Œå®¶æ•°ç»Ÿè®¡ã€‚"""
    rise = int((df["æ¶¨è·Œå¹…"] > 0).sum())
    fall = int((df["æ¶¨è·Œå¹…"] < 0).sum())
    flat = int((df["æ¶¨è·Œå¹…"] == 0).sum())
    return {"ä¸Šæ¶¨": rise, "ä¸‹è·Œ": fall, "å¹³ç›˜": flat, "æ¶¨è·Œæ¯”": round(rise / max(fall, 1), 2), "æ€»æ•°": len(df)}

def fetch_zt_pool(date_str): return _safe_fetch("æ¶¨åœæ± ", lambda: ak.stock_zt_pool_em(date=date_str))
def fetch_zb_pool(date_str): return _safe_fetch("ç‚¸æ¿æ± ", lambda: ak.stock_zt_pool_zbgc_em(date=date_str))
def fetch_dt_pool(date_str): return _safe_fetch("è·Œåœæ± ", lambda: ak.stock_zt_pool_dtgc_em(date=date_str))
def fetch_previous_zt(date_str): return _safe_fetch("æ˜¨æ—¥æ¶¨åœæ± ", lambda: ak.stock_zt_pool_previous_em(date=date_str))


# ---------------------------------------------------------------------------
# åˆ†æ
# ---------------------------------------------------------------------------

def analyze_streak_tiers(zt_df):
    if zt_df.empty or "è¿æ¿æ•°" not in zt_df.columns:
        return []
    tiers = []
    for streak, grp in zt_df.groupby("è¿æ¿æ•°"):
        tiers.append({"æ¿æ•°": int(streak), "å®¶æ•°": len(grp),
                       "ä»£è¡¨ä¸ªè‚¡": "ã€".join(grp["åç§°"].head(3).tolist())})
    tiers.sort(key=lambda x: x["æ¿æ•°"], reverse=True)
    return tiers

def analyze_top_industries(zt_df):
    if zt_df.empty or "æ‰€å±è¡Œä¸š" not in zt_df.columns:
        return []
    ic = zt_df.groupby("æ‰€å±è¡Œä¸š").agg(
        æ¶¨åœå®¶æ•°=("ä»£ç ", "count"),
        ä»£è¡¨ä¸ªè‚¡=("åç§°", lambda x: "ã€".join(x.head(2))),
    ).sort_values("æ¶¨åœå®¶æ•°", ascending=False).head(5).reset_index()
    return ic.to_dict(orient="records")


def _time_to_seconds(t: str) -> int:
    """å°†å°æ¿æ—¶é—´å­—ç¬¦ä¸²(HHMMSS)è½¬æ¢ä¸ºå½“æ—¥ç§’æ•°ã€‚"""
    s = str(t).replace(":", "").strip().ljust(6, "0")
    try:
        return int(s[:2]) * 3600 + int(s[2:4]) * 60 + int(s[4:6])
    except (ValueError, IndexError):
        return 0


def classify_board_type(first_seal, last_seal) -> str:
    """æ ¹æ®é¦–æ¬¡/æœ€åå°æ¿æ—¶é—´æ¨æ–­æ¿å‹ã€‚

    ä¸€å­—æ¿: ç«ä»·å³å°å…¨å¤©ä¸å¼€  |  Tå­—æ¿: ç«ä»·å°æ¿ç›˜ä¸­å¼€æ¿åå›å°
    ç§’æ¿: å¼€ç›˜æ•°åˆ†é’Ÿå†…å°æ­»    |  æ¢æ‰‹æ¿: ç»å……åˆ†æ¢æ‰‹åå°æ¿
    çƒ‚æ¿: å¤šæ¬¡ç‚¸å¼€å†å°        |  å°¾ç›˜æ¿: 14:00åå°æ¿
    """
    if not first_seal or not last_seal:
        return "æœªçŸ¥"
    fs = _time_to_seconds(first_seal)
    ls = _time_to_seconds(last_seal)
    if fs == 0:
        return "æœªçŸ¥"
    gap = ls - fs
    auction_cutoff = 9 * 3600 + 25 * 60 + 2  # 09:25:02

    if fs <= auction_cutoff:
        return "ä¸€å­—æ¿" if gap <= 2 else "Tå­—æ¿"
    if fs <= 9 * 3600 + 35 * 60:
        return "ç§’æ¿" if gap <= 300 else "åˆ†æ­§æ¿"
    if ls >= 14 * 3600:
        return "å°¾ç›˜æ¿" if fs >= 14 * 3600 else "çƒ‚æ¿"
    return "æ¢æ‰‹æ¿" if gap <= 600 else "åˆ†æ­§æ¿"


def calc_volume_analysis(current_vol: float, date_str: str) -> dict:
    """ä¸å†å²ç¼“å­˜å¯¹æ¯”ï¼Œè®¡ç®—é‡èƒ½åˆ†ææ•°æ®ã€‚"""
    result: dict = {}
    if not DATA_DIR.exists() or current_vol <= 0:
        return result
    recent_vols: list[tuple[str, float]] = []
    for f in sorted(DATA_DIR.glob("*.json"), reverse=True):
        d = f.stem
        if not d.isdigit() or d >= date_str:
            continue
        try:
            with open(f, "r", encoding="utf-8") as fh:
                vol = json.load(fh).get("ä¸¤å¸‚æˆäº¤é¢_äº¿", 0)
            if vol > 0:
                recent_vols.append((d, vol))
        except Exception:
            pass
        if len(recent_vols) >= 5:
            break
    if recent_vols:
        prev_vol = recent_vols[0][1]
        result["æ˜¨æ—¥æˆäº¤é¢_äº¿"] = prev_vol
        result["æ—¥ç¯æ¯”%"] = round((current_vol / prev_vol - 1) * 100, 1)
    if len(recent_vols) >= 5:
        avg5 = sum(v for _, v in recent_vols[:5]) / 5
        result["5æ—¥å‡é‡_äº¿"] = round(avg5, 0)
        result["vs_5æ—¥å‡é‡%"] = round((current_vol / avg5 - 1) * 100, 1)
    return result


def enrich_details_with_spot(details: list[dict], spot_df: pd.DataFrame) -> list[dict]:
    """ç”¨å…¨Aå®æ—¶è¡Œæƒ…æ•°æ®è¡¥å……æ¶¨åœæ˜ç»†çš„æ¢æ‰‹ç‡/æŒ¯å¹…/é‡æ¯”ã€‚"""
    if spot_df.empty or not details:
        return details
    spot_map: dict[str, dict] = {}
    for col in ["æ¢æ‰‹ç‡", "æŒ¯å¹…", "é‡æ¯”"]:
        if col not in spot_df.columns:
            return details
    for _, row in spot_df.iterrows():
        code = str(row.get("ä»£ç ", ""))
        spot_map[code] = {
            "æ¢æ‰‹ç‡": round(float(row.get("æ¢æ‰‹ç‡", 0)), 2),
            "æŒ¯å¹…": round(float(row.get("æŒ¯å¹…", 0)), 2),
            "é‡æ¯”": round(float(row.get("é‡æ¯”", 0)), 2),
        }
    for d in details:
        code = str(d.get("ä»£ç ", ""))
        if code in spot_map:
            d.update(spot_map[code])
    return details


# é‡èƒ½å¼‚åŠ¨ï¼šæœªæ¶¨åœä½†é‡æ¯”>=è¯¥é˜ˆå€¼è§†ä¸ºå¼‚å¸¸æ”¾é‡
VOLUME_ANOMALY_LIANGBI_MIN = 2.0
VOLUME_ANOMALY_TOP_N = 50


def get_volume_anomaly_non_zt(spot_df: pd.DataFrame, zt_codes: set[str]) -> list[dict]:
    """ä»å…¨Aè¡Œæƒ…ä¸­ç­›å‡ºæœªæ¶¨åœä½†é‡èƒ½å¼‚å¸¸çš„ä¸ªè‚¡ï¼ˆé‡æ¯”é™åºï¼Œå–å‰ Nï¼‰ã€‚"""
    if spot_df.empty or "é‡æ¯”" not in spot_df.columns:
        return []
    need_cols = ["ä»£ç ", "åç§°", "æ¶¨è·Œå¹…", "é‡æ¯”", "æ¢æ‰‹ç‡", "æˆäº¤é¢"]
    missing = [c for c in need_cols if c not in spot_df.columns]
    if missing:
        return []
    df = spot_df.copy()
    df["é‡æ¯”"] = pd.to_numeric(df["é‡æ¯”"], errors="coerce").fillna(0)
    df["æ¶¨è·Œå¹…"] = pd.to_numeric(df["æ¶¨è·Œå¹…"], errors="coerce").fillna(0)
    # æ’é™¤æ¶¨åœï¼šæ¶¨è·Œå¹… < 9.5 æˆ– ä»£ç ä¸åœ¨æ¶¨åœæ± 
    df = df[df["æ¶¨è·Œå¹…"] < 9.5]
    df = df[~df["ä»£ç "].astype(str).isin(zt_codes)]
    df = df[df["é‡æ¯”"] >= VOLUME_ANOMALY_LIANGBI_MIN]
    df = df.sort_values("é‡æ¯”", ascending=False).head(VOLUME_ANOMALY_TOP_N)
    out = []
    for _, row in df.iterrows():
        try:
            vol = row.get("æˆäº¤é¢", 0)
            if hasattr(vol, "item"):
                vol = vol.item()
            vol = round(float(vol), 0) if vol else 0
        except (TypeError, ValueError):
            vol = 0
        out.append({
            "ä»£ç ": str(row["ä»£ç "]),
            "åç§°": str(row["åç§°"]),
            "æ¶¨è·Œå¹…": round(float(row["æ¶¨è·Œå¹…"]), 2),
            "é‡æ¯”": round(float(row["é‡æ¯”"]), 2),
            "æ¢æ‰‹ç‡": round(float(row.get("æ¢æ‰‹ç‡", 0)), 2),
            "æˆäº¤é¢": vol,
        })
    return out


# ---------------------------------------------------------------------------
# å•æ—¥é‡‡é›†
# ---------------------------------------------------------------------------

def collect_single(date_str: str, *, use_realtime: bool = True, index_hist: Optional[dict] = None) -> dict:
    """é‡‡é›†å•æ—¥æ•°æ®ã€‚use_realtime=True æ—¶ä½¿ç”¨å®æ—¶æ¥å£ï¼ˆé€‚åˆå½“æ—¥ï¼‰ï¼Œå¦åˆ™ç”¨å†å²æ¥å£ã€‚"""
    spot_df = pd.DataFrame()
    if use_realtime:
        index_data = fetch_index_data()
        spot_df = _safe_fetch("å…¨Aè¡Œæƒ…", fetch_a_spot)
        rf = calc_rise_fall_stats(spot_df) if not spot_df.empty else {
            "ä¸Šæ¶¨": 0, "ä¸‹è·Œ": 0, "å¹³ç›˜": 0, "æ¶¨è·Œæ¯”": 1.0, "æ€»æ•°": 0}
    else:
        index_data = (index_hist or {}).get(date_str, {})
        rf = {"ä¸Šæ¶¨": 0, "ä¸‹è·Œ": 0, "å¹³ç›˜": 0, "æ¶¨è·Œæ¯”": 1.0, "æ€»æ•°": 0}

    zt_df = fetch_zt_pool(date_str)
    zt_count = len(zt_df)
    zb_df = fetch_zb_pool(date_str)
    zb_count = len(zb_df)
    seal_rate = round(zt_count / max(zt_count + zb_count, 1) * 100, 1)

    dt_df = fetch_dt_pool(date_str)
    dt_count = len(dt_df)

    prev_df = fetch_previous_zt(date_str)
    if not prev_df.empty and "æ¶¨è·Œå¹…" in prev_df.columns:
        premium = round(prev_df["æ¶¨è·Œå¹…"].astype(float).mean(), 2)
    else:
        premium = 0.0

    max_streak = 0
    if not zt_df.empty and "è¿æ¿æ•°" in zt_df.columns:
        max_streak = int(zt_df["è¿æ¿æ•°"].max())

    tiers = analyze_streak_tiers(zt_df)
    top_industries = analyze_top_industries(zt_df)

    if not use_realtime:
        rf["æ¶¨è·Œæ¯”"] = round(zt_count / max(dt_count, 1), 2) if dt_count > 0 else 5.0

    total_score, dim_scores = calc_emotion_score(
        zt_count, seal_rate, premium, max_streak, rf["æ¶¨è·Œæ¯”"], dt_count)

    total_vol = 0
    if isinstance(index_data, dict):
        for name in ["ä¸Šè¯æŒ‡æ•°", "æ·±è¯æˆæŒ‡"]:
            v = index_data.get(name, {})
            total_vol += v.get("æˆäº¤é¢_äº¿", 0)

    # æ¶¨åœæ˜ç»†ï¼ˆå‰20 + å…¨éƒ¨è¿æ¿è‚¡ï¼‰+ æ¿å‹æ¨æ–­
    zt_details = []
    lianban_details = []
    if not zt_df.empty:
        cols = ["ä»£ç ", "åç§°", "æ¶¨è·Œå¹…", "è¿æ¿æ•°", "é¦–æ¬¡å°æ¿æ—¶é—´", "æœ€åå°æ¿æ—¶é—´", "å°æ¿èµ„é‡‘", "æ‰€å±è¡Œä¸š"]
        available = [c for c in cols if c in zt_df.columns]
        for _, row in zt_df[available].head(20).iterrows():
            d = row.to_dict()
            d["æ¿å‹"] = classify_board_type(d.get("é¦–æ¬¡å°æ¿æ—¶é—´"), d.get("æœ€åå°æ¿æ—¶é—´"))
            zt_details.append(d)
        if "è¿æ¿æ•°" in zt_df.columns:
            lb_df = zt_df[zt_df["è¿æ¿æ•°"] >= 2].sort_values("è¿æ¿æ•°", ascending=False)
            for _, row in lb_df[available].iterrows():
                d = row.to_dict()
                d["æ¿å‹"] = classify_board_type(d.get("é¦–æ¬¡å°æ¿æ—¶é—´"), d.get("æœ€åå°æ¿æ—¶é—´"))
                lianban_details.append(d)

    # ç”¨å…¨Aè¡Œæƒ…è¡¥å……è¿æ¿è‚¡çš„æ¢æ‰‹ç‡/æŒ¯å¹…/é‡æ¯”ï¼ˆä»…å®æ—¶æ¨¡å¼ï¼‰
    if use_realtime and not spot_df.empty:
        enrich_details_with_spot(zt_details, spot_df)
        enrich_details_with_spot(lianban_details, spot_df)

    # é‡èƒ½å¼‚åŠ¨ï¼šæœªæ¶¨åœä½†é‡æ¯”>=é˜ˆå€¼çš„ä¸ªè‚¡åˆ—è¡¨ï¼ˆä»…å•æ—¥/å®æ—¶æ¨¡å¼ï¼‰
    zt_codes = set(zt_df["ä»£ç "].astype(str)) if not zt_df.empty and "ä»£ç " in zt_df.columns else set()
    volume_anomaly = get_volume_anomaly_non_zt(spot_df, zt_codes) if use_realtime and not spot_df.empty else []

    # é‡èƒ½åˆ†æ
    vol_analysis = calc_volume_analysis(total_vol, date_str) if total_vol > 0 else {}

    return {
        "æ—¥æœŸ": date_str, "æŒ‡æ•°": index_data, "ä¸¤å¸‚æˆäº¤é¢_äº¿": total_vol,
        "é‡èƒ½åˆ†æ": vol_analysis, "æ¶¨è·Œç»Ÿè®¡": rf,
        "æ¶¨åœå®¶æ•°": zt_count, "ç‚¸æ¿å®¶æ•°": zb_count,
        "å°æ¿ç‡": seal_rate, "è·Œåœå®¶æ•°": dt_count, "æ˜¨æ¶¨åœæº¢ä»·ç‡": premium,
        "æœ€é«˜è¿æ¿": max_streak, "è¿æ¿æ¢¯é˜Ÿ": tiers, "æ¶¨åœè¡Œä¸šTOP5": top_industries,
        "æ¶¨åœæ˜ç»†_å‰20": zt_details, "è¿æ¿è‚¡æ˜ç»†": lianban_details,
        "é‡èƒ½å¼‚åŠ¨_æœªæ¶¨åœ": volume_anomaly,
        "æƒ…ç»ªå„ç»´åº¦": dim_scores,
        "æƒ…ç»ªç»¼åˆå¾—åˆ†": total_score, "æƒ…ç»ªé˜¶æ®µ": emotion_stage_full(total_score),
    }


# ---------------------------------------------------------------------------
# æ‰¹é‡é‡‡é›†
# ---------------------------------------------------------------------------

def collect_batch(dates: list[str], *, force: bool = False) -> list[dict]:
    """æ‰¹é‡é‡‡é›†å¤šä¸ªäº¤æ˜“æ—¥æ•°æ®ï¼Œæ”¯æŒå¢é‡æ›´æ–°ã€‚"""
    DATA_DIR.mkdir(exist_ok=True)
    today_str = datetime.now().strftime("%Y%m%d")

    to_collect = []
    cached = []
    for d in dates:
        json_path = DATA_DIR / f"{d}.json"
        if not force and json_path.exists():
            cached.append(d)
        else:
            to_collect.append(d)

    if cached:
        print(f"ğŸ“¦ å·²ç¼“å­˜ {len(cached)} å¤©ï¼ˆè·³è¿‡ï¼‰: {', '.join(cached[:5])}{'...' if len(cached) > 5 else ''}")
    if not to_collect:
        print("âœ… æ‰€æœ‰æ—¥æœŸå·²æœ‰ç¼“å­˜ï¼Œæ— éœ€é‡‡é›†")
        return _load_cached(dates)

    print(f"ğŸ“Š éœ€é‡‡é›† {len(to_collect)} ä¸ªäº¤æ˜“æ—¥\n")

    hist_dates = [d for d in to_collect if d != today_str]
    is_today_in_list = today_str in to_collect

    index_hist = {}
    if hist_dates:
        print("  è·å–æŒ‡æ•°å†å²æ•°æ®...")
        index_hist = fetch_index_hist(hist_dates)

    results = []
    total = len(to_collect)
    for i, d in enumerate(to_collect, 1):
        print(f"  [{i}/{total}] é‡‡é›† {d}...", end=" ", flush=True)
        use_rt = (d == today_str)
        try:
            data = collect_single(d, use_realtime=use_rt, index_hist=index_hist)
            if data["æ¶¨åœå®¶æ•°"] == 0 and data["è·Œåœå®¶æ•°"] == 0:
                print("â­ æ— æ•°æ®ï¼ˆéäº¤æ˜“æ—¥ï¼Ÿï¼‰")
                continue
            _save_json(data, d)
            print("âœ…")
            results.append(data)
        except Exception as e:
            print(f"âŒ {e}")

    for d in cached:
        loaded = _load_single(d)
        if loaded:
            results.append(loaded)

    results.sort(key=lambda x: x["æ—¥æœŸ"])
    return results


def _load_cached(dates: list[str]) -> list[dict]:
    results = []
    for d in dates:
        data = _load_single(d)
        if data:
            results.append(data)
    results.sort(key=lambda x: x["æ—¥æœŸ"])
    return results


def _load_single(date_str: str) -> dict | None:
    path = DATA_DIR / f"{date_str}.json"
    if path.exists():
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return None


# ---------------------------------------------------------------------------
# å‘¨æœŸæ±‡æ€»æŠ¥å‘Š
# ---------------------------------------------------------------------------

def _compat(r: dict) -> dict:
    """å…¼å®¹æ–°æ—§ JSON æ ¼å¼çš„ key å·®å¼‚ã€‚"""
    if "æƒ…ç»ªç»¼åˆå¾—åˆ†" not in r and "æƒ…ç»ªå¾—åˆ†" in r:
        r["æƒ…ç»ªç»¼åˆå¾—åˆ†"] = r["æƒ…ç»ªå¾—åˆ†"]
    if "æƒ…ç»ªå„ç»´åº¦" not in r and "ç»´åº¦" in r:
        r["æƒ…ç»ªå„ç»´åº¦"] = r["ç»´åº¦"]
    for key in ["æ¶¨åœå®¶æ•°", "ç‚¸æ¿å®¶æ•°", "å°æ¿ç‡", "è·Œåœå®¶æ•°", "æ˜¨æ¶¨åœæº¢ä»·ç‡", "æœ€é«˜è¿æ¿", "æƒ…ç»ªç»¼åˆå¾—åˆ†"]:
        r.setdefault(key, 0)
    r.setdefault("æƒ…ç»ªé˜¶æ®µ", emotion_stage_full(r.get("æƒ…ç»ªç»¼åˆå¾—åˆ†", 0)))
    # å…¼å®¹æ—§æ ¼å¼æŒ‡æ•°: {name: {close, pct, volume}} â†’ {name: {æ”¶ç›˜, æ¶¨è·Œå¹…, æˆäº¤é¢_äº¿}}
    idx = r.get("æŒ‡æ•°", {})
    if isinstance(idx, dict):
        for name, v in idx.items():
            if isinstance(v, dict) and "close" in v and "æ”¶ç›˜" not in v:
                v["æ”¶ç›˜"] = v.pop("close")
                v["æ¶¨è·Œå¹…"] = v.pop("pct", 0)
                vol = v.pop("volume", 0)
                if vol and "æˆäº¤é¢_äº¿" not in v:
                    v["æˆäº¤é¢_äº¿"] = round(vol / 1e8, 0) if vol > 1e6 else vol
    r.setdefault("æŒ‡æ•°", {})
    # æ—§æ ¼å¼æ¶¨åœè¡Œä¸šTOP5: dict {è¡Œä¸š: æ•°é‡} â†’ æ–°æ ¼å¼: list of {æ‰€å±è¡Œä¸š, æ¶¨åœå®¶æ•°, ä»£è¡¨ä¸ªè‚¡}
    ind = r.get("æ¶¨åœè¡Œä¸šTOP5", [])
    if isinstance(ind, dict):
        new_ind = [{"æ‰€å±è¡Œä¸š": k, "æ¶¨åœå®¶æ•°": v, "ä»£è¡¨ä¸ªè‚¡": ""} for k, v in ind.items()]
        new_ind.sort(key=lambda x: x["æ¶¨åœå®¶æ•°"], reverse=True)
        r["æ¶¨åœè¡Œä¸šTOP5"] = new_ind
    elif not isinstance(ind, list):
        r["æ¶¨åœè¡Œä¸šTOP5"] = []
    # æ—§æ ¼å¼è¿æ¿æ¢¯é˜Ÿ: dict {str_æ¿æ•°: {å®¶æ•°, ä»£è¡¨}} â†’ æ–°æ ¼å¼: list of {æ¿æ•°, å®¶æ•°, ä»£è¡¨ä¸ªè‚¡}
    tiers = r.get("è¿æ¿æ¢¯é˜Ÿ", [])
    if isinstance(tiers, dict):
        new_tiers = []
        for k, v in tiers.items():
            new_tiers.append({
                "æ¿æ•°": int(k),
                "å®¶æ•°": v.get("å®¶æ•°", 0),
                "ä»£è¡¨ä¸ªè‚¡": v.get("ä»£è¡¨", v.get("ä»£è¡¨ä¸ªè‚¡", "")),
            })
        new_tiers.sort(key=lambda x: x["æ¿æ•°"], reverse=True)
        r["è¿æ¿æ¢¯é˜Ÿ"] = new_tiers
    elif not isinstance(tiers, list):
        r["è¿æ¿æ¢¯é˜Ÿ"] = []
    return r


def generate_summary(dates: list[str], data_dir: Path) -> str:
    """ç”Ÿæˆå‘¨æœŸæ±‡æ€»æŠ¥å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰ï¼ŒåŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°å¹¶ä¿å­˜æ–‡ä»¶ã€‚"""
    records = []
    for d in dates:
        data = _load_single(d)
        if data:
            records.append(_compat(data))
    if not records:
        return "æ— å¯ç”¨æ•°æ®"

    records.sort(key=lambda x: x["æ—¥æœŸ"])
    start = records[0]["æ—¥æœŸ"]
    end = records[-1]["æ—¥æœŸ"]
    lines = []

    lines.append(f"# Aè‚¡å‘¨æœŸå¤ç›˜æ±‡æ€» â€”â€” {start[:4]}/{start[4:6]}/{start[6:]} ~ {end[:4]}/{end[4:6]}/{end[6:]}")
    lines.append("")

    # åŒºé—´ç»Ÿè®¡
    zt_counts = [r["æ¶¨åœå®¶æ•°"] for r in records]
    scores = [r["æƒ…ç»ªç»¼åˆå¾—åˆ†"] for r in records]
    avg_zt = sum(zt_counts) / len(zt_counts)
    avg_score = sum(scores) / len(scores)
    max_s = max(scores)
    min_s = min(scores)
    max_d = next(r["æ—¥æœŸ"] for r in records if r["æƒ…ç»ªç»¼åˆå¾—åˆ†"] == max_s)
    min_d = next(r["æ—¥æœŸ"] for r in records if r["æƒ…ç»ªç»¼åˆå¾—åˆ†"] == min_s)

    idx_first = records[0].get("æŒ‡æ•°", {}).get("ä¸Šè¯æŒ‡æ•°", {})
    idx_last = records[-1].get("æŒ‡æ•°", {}).get("ä¸Šè¯æŒ‡æ•°", {})
    first_close = idx_first.get("æ”¶ç›˜", 0)
    last_close = idx_last.get("æ”¶ç›˜", 0)
    period_ret = round((last_close / first_close - 1) * 100, 2) if first_close else 0

    lines.append("## åŒºé—´ç»Ÿè®¡")
    lines.append("")
    lines.append(f"| æŒ‡æ ‡ | æ•°å€¼ |")
    lines.append(f"|------|------|")
    lines.append(f"| äº¤æ˜“å¤©æ•° | {len(records)} å¤© |")
    lines.append(f"| æ—¥å‡æ¶¨åœ | {avg_zt:.1f} å®¶ |")
    lines.append(f"| å¹³å‡æƒ…ç»ªå¾—åˆ† | {avg_score:.2f} åˆ† |")
    lines.append(f"| æœ€é«˜æƒ…ç»ª | {max_s:.2f}ï¼ˆ{max_d}ï¼‰ |")
    lines.append(f"| æœ€ä½æƒ…ç»ª | {min_s:.2f}ï¼ˆ{min_d}ï¼‰ |")
    lines.append(f"| ä¸Šè¯åŒºé—´æ¶¨å¹… | {period_ret:+.2f}%ï¼ˆ{first_close} â†’ {last_close}ï¼‰ |")
    lines.append("")

    # æŒ‡æ•°èµ°åŠ¿è¡¨
    lines.append("## ä¸‰å¤§æŒ‡æ•°èµ°åŠ¿")
    lines.append("")
    lines.append("| æ—¥æœŸ | ä¸Šè¯æŒ‡æ•° | æ·±è¯æˆæŒ‡ | åˆ›ä¸šæ¿æŒ‡ |")
    lines.append("|------|----------|----------|----------|")
    for r in records:
        d = r["æ—¥æœŸ"]
        idx = r.get("æŒ‡æ•°", {})
        cells = []
        for name in ["ä¸Šè¯æŒ‡æ•°", "æ·±è¯æˆæŒ‡", "åˆ›ä¸šæ¿æŒ‡"]:
            v = idx.get(name, {})
            c = v.get("æ”¶ç›˜", "--")
            p = v.get("æ¶¨è·Œå¹…", 0)
            cells.append(f"{c}ï¼ˆ{p:+.2f}%ï¼‰" if c != "--" else "--")
        lines.append(f"| {d} | {cells[0]} | {cells[1]} | {cells[2]} |")
    lines.append("")

    # æƒ…ç»ªæ ¸å¿ƒæ•°æ®è¡¨
    lines.append("## æ¯æ—¥æƒ…ç»ªæ ¸å¿ƒæ•°æ®")
    lines.append("")
    lines.append("| æ—¥æœŸ | æ¶¨åœ | ç‚¸æ¿ | å°æ¿ç‡ | è·Œåœ | æº¢ä»·ç‡ | æœ€é«˜è¿æ¿ | å¾—åˆ† | é˜¶æ®µ |")
    lines.append("|------|------|------|--------|------|--------|---------|------|------|")
    for r in records:
        stage = emotion_stage(r["æƒ…ç»ªç»¼åˆå¾—åˆ†"])
        lines.append(
            f"| {r['æ—¥æœŸ']} | {r['æ¶¨åœå®¶æ•°']} | {r['ç‚¸æ¿å®¶æ•°']} | {r['å°æ¿ç‡']}% "
            f"| {r['è·Œåœå®¶æ•°']} | {r['æ˜¨æ¶¨åœæº¢ä»·ç‡']:+.2f}% | {r['æœ€é«˜è¿æ¿']}æ¿ "
            f"| {r['æƒ…ç»ªç»¼åˆå¾—åˆ†']:.2f} | {stage} |"
        )
    lines.append("")

    # æƒ…ç»ªèµ°åŠ¿æ›²çº¿
    lines.append("## æƒ…ç»ªèµ°åŠ¿æ›²çº¿")
    lines.append("")
    lines.append("```")
    for r in records:
        s = r["æƒ…ç»ªç»¼åˆå¾—åˆ†"]
        bar_len = int(s / 10 * 40)
        bar = "â–ˆ" * bar_len + "â–‘" * (40 - bar_len)
        stage = emotion_stage(s)
        lines.append(f"  {r['æ—¥æœŸ']} {bar} {s:.2f} {stage}")
    lines.append("```")
    lines.append("")

    # é¾™å¤´æ¼”è¿›è¿½è¸ª
    lines.append("## é¾™å¤´æ¼”è¿›è¿½è¸ª")
    lines.append("")
    stock_history = defaultdict(list)
    for r in records:
        for tier in r.get("è¿æ¿æ¢¯é˜Ÿ", []):
            if tier["æ¿æ•°"] >= 2:
                for name in tier["ä»£è¡¨ä¸ªè‚¡"].split("ã€"):
                    name = name.strip()
                    if name:
                        stock_history[name].append((r["æ—¥æœŸ"], tier["æ¿æ•°"]))

    leaders = {name: hist for name, hist in stock_history.items() if len(hist) >= 2}
    if leaders:
        leaders_sorted = sorted(leaders.items(), key=lambda x: max(b for _, b in x[1]), reverse=True)
        lines.append("| é¾™å¤´ | æ¼”è¿›è½¨è¿¹ | æœ€é«˜æ¿æ•° |")
        lines.append("|------|----------|---------|")
        for name, hist in leaders_sorted[:10]:
            trail = " â†’ ".join(f"{d[4:6]}/{d[6:]}({b}æ¿)" for d, b in hist)
            max_b = max(b for _, b in hist)
            lines.append(f"| {name} | {trail} | {max_b}æ¿ |")
    else:
        lines.append("_æœ¬å‘¨æœŸå†…æ— è·¨æ—¥è¿æ¿é¾™å¤´_")
    lines.append("")

    # é¢˜æè½®åŠ¨æ±‡æ€»
    lines.append("## é¢˜æè½®åŠ¨")
    lines.append("")
    lines.append("| æ—¥æœŸ | TOP1 è¡Œä¸š | TOP2 è¡Œä¸š | TOP3 è¡Œä¸š |")
    lines.append("|------|-----------|-----------|-----------|")
    for r in records:
        tops = r.get("æ¶¨åœè¡Œä¸šTOP5", [])
        cells = []
        for i in range(3):
            if i < len(tops):
                t = tops[i]
                cells.append(f"{t['æ‰€å±è¡Œä¸š']}({t['æ¶¨åœå®¶æ•°']})")
            else:
                cells.append("--")
        lines.append(f"| {r['æ—¥æœŸ']} | {cells[0]} | {cells[1]} | {cells[2]} |")
    lines.append("")

    md = "\n".join(lines)

    # ä¿å­˜æ–‡ä»¶
    data_dir.mkdir(exist_ok=True)
    out_path = data_dir / f"summary_{start}_{end}.md"
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(md)

    # æ‰“å°åˆ°æ§åˆ¶å°
    print(f"\n{'=' * 80}")
    print(md)
    print(f"{'=' * 80}")
    print(f"\nâœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜è‡³: {out_path}")

    return md


# ---------------------------------------------------------------------------
# å¤ç›˜è‰ç¨¿ç”Ÿæˆ
# ---------------------------------------------------------------------------

REVIEW_TEMPLATE_DIR = Path(__file__).resolve().parent.parent / "å¤ç›˜æ¨¡æ¿"
DAILY_REVIEW_DIR = Path(__file__).resolve().parent.parent / "æ¯æ—¥å¤ç›˜"
WEEKDAY_CN = ["ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "æ—¥"]


def _get_prev_trading_days(date_str: str, n: int) -> list[str]:
    """è¿”å› date_str åŠä¹‹å‰å…± n ä¸ªäº¤æ˜“æ—¥çš„åˆ—è¡¨ï¼ˆå« date_strï¼‰ï¼ŒæŒ‰æ—¶é—´æ­£åºã€‚"""
    try:
        dt = datetime.strptime(date_str, "%Y%m%d")
    except ValueError:
        return []
    start = (dt - timedelta(days=max(n * 3, 60))).strftime("%Y%m%d")
    all_days = get_trading_days(start, date_str)
    return all_days[-n:] if len(all_days) >= n else all_days


def generate_draft_review(
    date_str: str,
    data_dict: dict,
    template_path: Optional[Path] = None,
    out_dir: Optional[Path] = None,
) -> Path:
    """æ ¹æ®å½“æ—¥é‡‡é›†æ•°æ®ç”Ÿæˆå¤ç›˜æ€»ç»“è‰ç¨¿ Markdownï¼Œå†™å…¥ æ¯æ—¥å¤ç›˜/YYYYMM/YYYY-MM-DD_draft.mdã€‚"""
    out_dir = out_dir or DAILY_REVIEW_DIR
    yyyymm = f"{date_str[:4]}{date_str[4:6]}"
    day_slug = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
    try:
        wd = datetime.strptime(date_str, "%Y%m%d").weekday()
        week_cn = WEEKDAY_CN[wd]
    except ValueError:
        week_cn = "X"
    out_path = out_dir / yyyymm / f"{day_slug}_draft.md"
    out_path.parent.mkdir(parents=True, exist_ok=True)

    # å‰ä¸¤æ—¥æ•°æ®ï¼ˆç”¨äº 3 æ—¥è¶‹åŠ¿ï¼‰
    prev3 = _get_prev_trading_days(date_str, 3)
    scores_3d = []
    for d in prev3:
        if d == date_str:
            scores_3d.append(data_dict.get("æƒ…ç»ªç»¼åˆå¾—åˆ†", data_dict.get("æƒ…ç»ªå¾—åˆ†", 0)))
        else:
            loaded = _load_single(d)
            if loaded:
                loaded = _compat(loaded)
                scores_3d.append(loaded.get("æƒ…ç»ªç»¼åˆå¾—åˆ†", 0))
            else:
                scores_3d.append(0)
    if len(scores_3d) == 3:
        s0, s1, s2 = scores_3d
        if s0 < s1 < s2:
            trend_3d = "è¿ç»­å‡æ¸©"
        elif s0 > s1 > s2:
            trend_3d = "è¿ç»­é™æ¸©"
        elif s0 > s1 and s1 < s2:
            trend_3d = "å…ˆé™åå‡"
        elif s0 < s1 and s1 > s2:
            trend_3d = "å…ˆå‡åé™"
        else:
            trend_3d = "æŒå¹³æˆ–éœ‡è¡"
    else:
        trend_3d = "____ï¼ˆä¸è¶³3æ—¥æ•°æ®ï¼‰"
    score_prev2 = scores_3d[0] if len(scores_3d) >= 3 else "____"
    score_prev1 = scores_3d[1] if len(scores_3d) >= 2 else "____"
    score_today = data_dict.get("æƒ…ç»ªç»¼åˆå¾—åˆ†", "____")

    idx = data_dict.get("æŒ‡æ•°", {})
    vol = data_dict.get("é‡èƒ½åˆ†æ", {})
    rf = data_dict.get("æ¶¨è·Œç»Ÿè®¡", {})

    def idx_cell(name: str) -> str:
        v = idx.get(name, {})
        c = v.get("æ”¶ç›˜", "____")
        p = v.get("æ¶¨è·Œå¹…", 0)
        if c == "____":
            return "____ ç‚¹ï¼ˆ____%ï¼‰"
        return f"{c} ç‚¹ï¼ˆ{p:+.2f}%ï¼‰"

    vol_note = "____"
    if vol.get("æ—¥ç¯æ¯”%") is not None:
        vol_note = f"è¾ƒæ˜¨æ—¥ {'æ”¾é‡' if vol['æ—¥ç¯æ¯”%'] > 0 else 'ç¼©é‡'} {vol['æ—¥ç¯æ¯”%']:+.1f}%"
    vol_5 = "____"
    if vol.get("vs_5æ—¥å‡é‡%") is not None:
        vol_5 = "æ”¾é‡" if vol["vs_5æ—¥å‡é‡%"] > 5 else ("ç¼©é‡" if vol["vs_5æ—¥å‡é‡%"] < -5 else "æŒå¹³")

    lines = [
        f"# æ¯æ—¥æƒ…ç»ªå¤ç›˜ - {day_slug[:4]}/{day_slug[5:7]}/{day_slug[8:10]}ï¼ˆæ˜ŸæœŸ{week_cn}ï¼‰",
        "",
        "---",
        "",
        "## ä¸€ã€å¤§ç›˜æ¦‚è§ˆ",
        "",
        "| æŒ‡æ ‡ | æ•°å€¼ | å¤‡æ³¨ |",
        "|------|------|------|",
        f"| ä¸Šè¯æŒ‡æ•° | {idx_cell('ä¸Šè¯æŒ‡æ•°')} | 5æ—¥çº¿ä¸Šæ–¹ / ä¸‹æ–¹ |",
        f"| æ·±æˆæŒ‡ | {idx_cell('æ·±è¯æˆæŒ‡')} | 5æ—¥çº¿ä¸Šæ–¹ / ä¸‹æ–¹ |",
        f"| åˆ›ä¸šæ¿æŒ‡ | {idx_cell('åˆ›ä¸šæ¿æŒ‡')} | 5æ—¥çº¿ä¸Šæ–¹ / ä¸‹æ–¹ |",
        f"| ä¸¤å¸‚æˆäº¤é¢ | {data_dict.get('ä¸¤å¸‚æˆäº¤é¢_äº¿', '____')} äº¿ | {vol_note} |",
        f"| 5æ—¥å‡é‡ | {vol.get('5æ—¥å‡é‡_äº¿', '____')} äº¿ | ä»Šæ—¥æˆäº¤é¢ vs 5æ—¥å‡é‡ï¼š{vol_5} |",
        f"| ä¸Šæ¶¨å®¶æ•° | {rf.get('ä¸Šæ¶¨', '____')} å®¶ | |",
        f"| ä¸‹è·Œå®¶æ•° | {rf.get('ä¸‹è·Œ', '____')} å®¶ | |",
        f"| æ¶¨è·Œæ¯” | {rf.get('æ¶¨è·Œæ¯”', '____')} | >2 æ™®æ¶¨ / 1-2 åå¼º / 0.5-1 åå¼± / <0.5 æ™®è·Œ |",
        "",
        "**å¤§ç›˜æŠ€æœ¯ä½ç½®**ï¼š",
        "- ä¸Šè¯ï¼š__æ—¥å‡çº¿é™„è¿‘ï¼ˆæ”¯æ’‘/å‹åŠ›ä½ ____ ç‚¹ï¼‰",
        "- åˆ›ä¸šæ¿ï¼š__æ—¥å‡çº¿é™„è¿‘ï¼ˆæ”¯æ’‘/å‹åŠ›ä½ ____ ç‚¹ï¼‰",
        "- å¤§ç›˜å‘¨æœŸåˆ¤æ–­ï¼šä¸Šå‡è¶‹åŠ¿ / éœ‡è¡ / ä¸‹é™è¶‹åŠ¿",
        "",
        "---",
        "",
        "## äºŒã€ç«ä»·å¤ç›˜",
        "",
        "ï¼ˆè¯·æ ¹æ®ç›˜é¢è¡¥å……ï¼šæ ¸å¿ƒè‚¡ç«ä»·è¡¨ç°ã€ç«ä»·æ•´ä½“æ°›å›´ï¼‰",
        "",
        "---",
        "",
        "## ä¸‰ã€æƒ…ç»ªæ ¸å¿ƒæ•°æ®",
        "",
    ]

    dim = data_dict.get("æƒ…ç»ªå„ç»´åº¦", {})
    lines.extend([
        "| æŒ‡æ ‡ | åŸå§‹å€¼ | è¯„åˆ†(1-10) | è¯„åˆ†å‚è€ƒ |",
        "|------|--------|-----------|----------|",
        f"| æ¶¨åœå®¶æ•° | {data_dict.get('æ¶¨åœå®¶æ•°', '____')} å®¶ | {dim.get('æ¶¨åœå®¶æ•°', '____')} | <20â†’1-2 / 20-40â†’3-4 / ... |",
        f"| å°æ¿ç‡ | {data_dict.get('å°æ¿ç‡', '____')}% | {dim.get('å°æ¿ç‡', '____')} | <40%â†’1-3 / ... |",
        f"| æ˜¨æ¶¨åœä»Šæ—¥æº¢ä»·ç‡ | {data_dict.get('æ˜¨æ¶¨åœæº¢ä»·ç‡', '____')}% | {dim.get('æ˜¨æ¶¨åœæº¢ä»·', '____')} | <-5%â†’1 / ... |",
        f"| æœ€é«˜è¿æ¿ | {data_dict.get('æœ€é«˜è¿æ¿', '____')} æ¿ | {dim.get('è¿æ¿é«˜åº¦', '____')} | æ— â†’1 / 2æ¿â†’3 / ... |",
        f"| æ¶¨è·Œæ¯” | {rf.get('æ¶¨è·Œæ¯”', '____')} | {dim.get('æ¶¨è·Œæ¯”', '____')} | <0.3â†’1 / ... |",
        f"| è·Œåœå®¶æ•° | {data_dict.get('è·Œåœå®¶æ•°', '____')} å®¶ | {dim.get('è·ŒåœåæŒ‡', '____')} | >30â†’1 / ... |",
        "",
        "### äºé’±æ•ˆåº”è¿½è¸ª",
        "",
        "| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |",
        "|------|------|------|",
        "| å¤§é¢è‚¡æ•°é‡ | ____ å®¶ | æ˜¨æ—¥æ¶¨åœ/è¿æ¿ä»Šæ—¥è·Œå¹… > 5% çš„ä¸ªè‚¡ |",
        "| æ˜¨æ¶¨åœå¤§é¢æ¯”ä¾‹ | ____% | å¤§é¢è‚¡ / æ˜¨æ—¥æ¶¨åœæ€»æ•° |",
        f"| ç‚¸æ¿è‚¡æ•°é‡ | {data_dict.get('ç‚¸æ¿å®¶æ•°', '____')} å®¶ | ç›˜ä¸­è§¦åŠæ¶¨åœä½†æœªå°ä½ |",
        "",
        "### æƒ…ç»ªç»¼åˆå¾—åˆ†",
        "",
        f"**ä»Šæ—¥æƒ…ç»ªå¾—åˆ†ï¼š{score_today} åˆ†**",
        "",
        "### è¿æ¿æ¢¯é˜Ÿåˆ†å¸ƒ",
        "",
        "| æ¿æ•° | å®¶æ•° | ä»£è¡¨ä¸ªè‚¡ |",
        "|------|------|----------|",
    ])

    for t in data_dict.get("è¿æ¿æ¢¯é˜Ÿ", []):
        lines.append(f"| {t.get('æ¿æ•°', '')}æ¿ | {t.get('å®¶æ•°', '')} å®¶ | {t.get('ä»£è¡¨ä¸ªè‚¡', '')} |")
    if not data_dict.get("è¿æ¿æ¢¯é˜Ÿ"):
        lines.append("| ____ | ____ å®¶ | |")

    lines.extend([
        "",
        "---",
        "",
        "## å››ã€æƒ…ç»ªå‘¨æœŸå®šä½",
        "",
        "### å¾—åˆ†èµ°åŠ¿",
        "",
        f"- **å‰æ—¥æƒ…ç»ªå¾—åˆ†**ï¼š{score_prev2} åˆ†",
        f"- **æ˜¨æ—¥æƒ…ç»ªå¾—åˆ†**ï¼š{score_prev1} åˆ†",
        f"- **ä»Šæ—¥æƒ…ç»ªå¾—åˆ†**ï¼š{score_today} åˆ†",
        f"- **3æ—¥è¶‹åŠ¿**ï¼š{trend_3d}",
        "",
        "### å½“å‰æ‰€å¤„å‘¨æœŸé˜¶æ®µ",
        "",
        f"- å½“å‰é˜¶æ®µï¼š{data_dict.get('æƒ…ç»ªé˜¶æ®µ', '____')}",
        "",
        "---",
        "",
        "## äº”ã€é¾™å¤´æ¢³ç†",
        "",
        "ï¼ˆè¯·æ ¹æ®ç›˜é¢è¡¥å……ï¼šæ€»é¾™å¤´ã€è¡¥æ¶¨é¾™ã€å‰æ’åŠ©æ”»ã€é¾™å¤´æ¼”è¿›åˆ¤æ–­ï¼‰",
        "",
        "---",
        "",
        "## å…­ã€é¢˜ææ¿å—åˆ†æ",
        "",
        "### å½“æ—¥æœ€å¼ºé¢˜æ TOP3",
        "",
        "| æ’å | é¢˜æåç§° | æ¶¨åœå®¶æ•° | ä»£è¡¨ä¸ªè‚¡ |",
        "|------|----------|----------|----------|",
    ])

    for i, ind in enumerate(data_dict.get("æ¶¨åœè¡Œä¸šTOP5", [])[:3], 1):
        lines.append(f"| {i} | {ind.get('æ‰€å±è¡Œä¸š', '')} | {ind.get('æ¶¨åœå®¶æ•°', '')} | {ind.get('ä»£è¡¨ä¸ªè‚¡', '')} |")
    for _ in range(3 - len(data_dict.get("æ¶¨åœè¡Œä¸šTOP5", [])[:3])):
        lines.append("| ____ | | | |")

    # é‡èƒ½å¼‚åŠ¨æœªæ¶¨åœ
    anomaly = data_dict.get("é‡èƒ½å¼‚åŠ¨_æœªæ¶¨åœ", [])
    if anomaly:
        lines.extend([
            "",
            "### é‡èƒ½å¼‚åŠ¨ï¼ˆæœªæ¶¨åœï¼‰",
            "",
            "| åç§° | æ¶¨è·Œå¹… | é‡æ¯” | æ¢æ‰‹ç‡ |",
            "|------|--------|------|--------|",
        ])
        for s in anomaly[:20]:
            lines.append(f"| {s.get('åç§°', '')} | {s.get('æ¶¨è·Œå¹…', 0):+.2f}% | {s.get('é‡æ¯”', '')} | {s.get('æ¢æ‰‹ç‡', '')}% |")
        if len(anomaly) > 20:
            lines.append(f"| ... å…± {len(anomaly)} åª | | | |")

    lines.extend([
        "",
        "---",
        "",
        "## ä¸ƒã€äº”æ—¥çº¿ä½å¸è·Ÿè¸ª",
        "",
        "ï¼ˆè¯·æ ¹æ®ç›˜é¢è¡¥å……ï¼šä½å¸å€™é€‰æ± ï¼‰",
        "",
        "---",
        "",
        "## å…«ã€æŒä»“ç®¡ç†",
        "",
        "ï¼ˆè¯·è¡¥å……ï¼šå½“å‰æŒä»“ã€æ˜æ—¥æ“ä½œè®¡åˆ’ï¼‰",
        "",
        "---",
        "",
        "## ä¹ã€æ˜æ—¥ç­–ç•¥",
        "",
        "ï¼ˆè¯·æ ¹æ®å¤ç›˜ç»“è®ºè¡¥å……ï¼šæƒ…æ™¯é¢„æ¡ˆã€ç«ä»·ç­–ç•¥ã€ä»“ä½è®¡åˆ’ã€å…³æ³¨æ–¹å‘ã€é£é™©æç¤ºï¼‰",
        "",
        "---",
        "",
        "## åã€äº¤æ˜“çºªå¾‹è‡ªæ£€",
        "",
        "ï¼ˆè¯·è¡¥å……ï¼šä»Šæ—¥æ“ä½œå›é¡¾ã€çºªå¾‹æ£€æŸ¥ï¼‰",
        "",
        "---",
        "",
        "> **æœ¬ç¨¿ç”±è„šæœ¬è‡ªåŠ¨ç”Ÿæˆï¼Œè¯·åœ¨æ­¤åŸºç¡€ä¸Šè¡¥å……ä¸»è§‚åˆ¤æ–­ä¸æ“ä½œè®¡åˆ’ã€‚**",
        "",
    ])

    md = "\n".join(lines)
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(md)
    return out_path

def print_report(data: dict):
    d = data["æ—¥æœŸ"]
    print(f"\n{'='*60}")
    print(f"  Aè‚¡æ¯æ—¥å¤ç›˜æ•°æ® â€”â€” {d[:4]}/{d[4:6]}/{d[6:]}")
    print(f"{'='*60}\n")

    print("ã€ä¸€ã€å¤§ç›˜æ¦‚è§ˆã€‘")
    idx = data.get("æŒ‡æ•°", {})
    if isinstance(idx, dict):
        for name, v in idx.items():
            if isinstance(v, dict) and "æ”¶ç›˜" in v:
                print(f"  {name}: {v['æ”¶ç›˜']} ç‚¹ï¼ˆ{v['æ¶¨è·Œå¹…']:+.2f}%ï¼‰")
    print(f"  ä¸¤å¸‚æˆäº¤é¢: {data['ä¸¤å¸‚æˆäº¤é¢_äº¿']:.0f} äº¿")
    va = data.get("é‡èƒ½åˆ†æ", {})
    if va.get("æ—¥ç¯æ¯”%") is not None:
        label = "æ”¾é‡" if va["æ—¥ç¯æ¯”%"] > 0 else "ç¼©é‡"
        print(f"  é‡èƒ½: è¾ƒæ˜¨æ—¥{label} {va['æ—¥ç¯æ¯”%']:+.1f}%", end="")
        if va.get("vs_5æ—¥å‡é‡%") is not None:
            print(f" | vs 5æ—¥å‡é‡ {va['vs_5æ—¥å‡é‡%']:+.1f}%", end="")
        print()
    rf = data.get("æ¶¨è·Œç»Ÿè®¡", {})
    if rf.get("ä¸Šæ¶¨"):
        print(f"  ä¸Šæ¶¨ {rf['ä¸Šæ¶¨']} å®¶ / ä¸‹è·Œ {rf['ä¸‹è·Œ']} å®¶ / å¹³ç›˜ {rf['å¹³ç›˜']} å®¶")
        print(f"  æ¶¨è·Œæ¯”: {rf['æ¶¨è·Œæ¯”']}")

    print("\nã€äºŒã€æƒ…ç»ªæ ¸å¿ƒæ•°æ®ã€‘")
    print(f"  æ¶¨åœ: {data['æ¶¨åœå®¶æ•°']} å®¶ | ç‚¸æ¿: {data['ç‚¸æ¿å®¶æ•°']} å®¶ | å°æ¿ç‡: {data['å°æ¿ç‡']}%")
    print(f"  è·Œåœ: {data['è·Œåœå®¶æ•°']} å®¶")
    print(f"  æ˜¨æ¶¨åœæº¢ä»·ç‡: {data['æ˜¨æ¶¨åœæº¢ä»·ç‡']:+.2f}%")
    print(f"  æœ€é«˜è¿æ¿: {data['æœ€é«˜è¿æ¿']} æ¿")
    print(f"  â˜… æƒ…ç»ªç»¼åˆå¾—åˆ†: {data['æƒ…ç»ªç»¼åˆå¾—åˆ†']} åˆ†")
    print(f"  â˜… å½“å‰é˜¶æ®µ: {data['æƒ…ç»ªé˜¶æ®µ']}")

    if data.get("è¿æ¿æ¢¯é˜Ÿ"):
        print("\nã€ä¸‰ã€è¿æ¿æ¢¯é˜Ÿã€‘")
        for t in data["è¿æ¿æ¢¯é˜Ÿ"]:
            print(f"  {t['æ¿æ•°']}æ¿: {t['å®¶æ•°']}å®¶ â†’ {t['ä»£è¡¨ä¸ªè‚¡']}")

    if data.get("è¿æ¿è‚¡æ˜ç»†"):
        print("\nã€å››ã€è¿æ¿è‚¡ç›˜å£æ˜ç»†ã€‘")
        for s in data["è¿æ¿è‚¡æ˜ç»†"]:
            name = s.get("åç§°", "?")
            streak = s.get("è¿æ¿æ•°", "?")
            btype = s.get("æ¿å‹", "?")
            fs = s.get("é¦–æ¬¡å°æ¿æ—¶é—´", "")
            ls = s.get("æœ€åå°æ¿æ—¶é—´", "")
            seal_money = s.get("å°æ¿èµ„é‡‘", 0)
            seal_yi = round(seal_money / 1e8, 1) if seal_money else 0
            turnover = s.get("æ¢æ‰‹ç‡", "")
            amplitude = s.get("æŒ¯å¹…", "")
            line = f"  {name}({streak}æ¿) [{btype}] é¦–å°{fs} æœ«å°{ls} å°å•{seal_yi}äº¿"
            if turnover:
                line += f" æ¢æ‰‹{turnover}%"
            if amplitude:
                line += f" æŒ¯å¹…{amplitude}%"
            print(line)

    if data.get("æ¶¨åœè¡Œä¸šTOP5"):
        print("\nã€äº”ã€æ¶¨åœè¡Œä¸š TOP5ã€‘")
        for i, ind in enumerate(data["æ¶¨åœè¡Œä¸šTOP5"], 1):
            print(f"  {i}. {ind['æ‰€å±è¡Œä¸š']}ï¼ˆ{ind['æ¶¨åœå®¶æ•°']}å®¶æ¶¨åœï¼‰â†’ {ind['ä»£è¡¨ä¸ªè‚¡']}")

    if data.get("é‡èƒ½å¼‚åŠ¨_æœªæ¶¨åœ"):
        print("\nã€å…­ã€é‡èƒ½å¼‚åŠ¨ï¼ˆæœªæ¶¨åœï¼‰ã€‘")
        for s in data["é‡èƒ½å¼‚åŠ¨_æœªæ¶¨åœ"][:15]:
            print(f"  {s.get('åç§°', '?')} æ¶¨è·Œå¹…{s.get('æ¶¨è·Œå¹…', 0):+.2f}% é‡æ¯”{s.get('é‡æ¯”', 0)} æ¢æ‰‹{s.get('æ¢æ‰‹ç‡', 0)}%")
        if len(data["é‡èƒ½å¼‚åŠ¨_æœªæ¶¨åœ"]) > 15:
            print(f"  ... å…± {len(data['é‡èƒ½å¼‚åŠ¨_æœªæ¶¨åœ'])} åªï¼ˆé‡æ¯”â‰¥{VOLUME_ANOMALY_LIANGBI_MIN}ï¼‰")

    print(f"\n{'='*60}\n")


def _save_json(data: dict, date_str: str):
    DATA_DIR.mkdir(exist_ok=True)
    out_file = DATA_DIR / f"{date_str}.json"
    def ser(obj):
        if isinstance(obj, pd.DataFrame): return obj.to_dict(orient="records")
        if isinstance(obj, (pd.Timestamp, datetime)): return str(obj)
        if hasattr(obj, "item"): return obj.item()
        return obj
    clean = json.loads(json.dumps(data, default=ser, ensure_ascii=False))
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(clean, f, ensure_ascii=False, indent=2)


# ---------------------------------------------------------------------------
# CLI å…¥å£
# ---------------------------------------------------------------------------

def parse_args():
    parser = argparse.ArgumentParser(
        description="Aè‚¡æ¯æ—¥å¤ç›˜æ•°æ®è‡ªåŠ¨é‡‡é›†è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument("date", nargs="?", help="å•æ—¥é‡‡é›†æ—¥æœŸï¼ˆYYYYMMDDï¼‰")
    parser.add_argument("--range", nargs=2, metavar=("START", "END"), help="æ‰¹é‡é‡‡é›†æ—¥æœŸèŒƒå›´")
    parser.add_argument("--days", type=int, help="é‡‡é›†æœ€è¿‘ N ä¸ªäº¤æ˜“æ—¥")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°é‡‡é›†ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰")
    parser.add_argument("--summary", action="store_true", help="ç”Ÿæˆå‘¨æœŸæ±‡æ€»æŠ¥å‘Š")
    parser.add_argument("--no-draft", dest="draft", action="store_false", default=True,
                        help="ä¸ç”Ÿæˆå¤ç›˜è‰ç¨¿ï¼ˆå•æ—¥/æ‰¹é‡å‡å¯ç”Ÿæˆè‰ç¨¿æ—¶é»˜è®¤ç”Ÿæˆï¼‰")
    parser.add_argument("--print-only", action="store_true", help="ä»…æ‰“å°ï¼Œä¸ä¿å­˜æ–‡ä»¶")
    return parser.parse_args()


def main():
    args = parse_args()

    # æ‰¹é‡æ¨¡å¼: --range
    if args.range:
        start, end = args.range
        dates = get_trading_days(start, end)
        if not dates:
            print(f"âŒ {start} ~ {end} èŒƒå›´å†…æ— äº¤æ˜“æ—¥")
            return
        print(f"ğŸ“… æ—¥æœŸèŒƒå›´ {start} ~ {end}ï¼Œå…± {len(dates)} ä¸ªäº¤æ˜“æ—¥\n")
        collect_batch(dates, force=args.force)
        if args.summary:
            generate_summary(dates, DATA_DIR)
        if args.draft:
            for d in dates:
                loaded = _load_single(d)
                if loaded:
                    p = generate_draft_review(d, loaded)
                    print(f"  è‰ç¨¿: {p}")
        return

    # æ‰¹é‡æ¨¡å¼: --days
    if args.days:
        dates = get_recent_trading_days(args.days)
        if not dates:
            print(f"âŒ æ— æ³•ç¡®å®šæœ€è¿‘ {args.days} ä¸ªäº¤æ˜“æ—¥")
            return
        print(f"ğŸ“… æœ€è¿‘ {args.days} ä¸ªäº¤æ˜“æ—¥: {dates[0]} ~ {dates[-1]}\n")
        collect_batch(dates, force=args.force)
        if args.summary:
            generate_summary(dates, DATA_DIR)
        if args.draft:
            for d in dates:
                loaded = _load_single(d)
                if loaded:
                    p = generate_draft_review(d, loaded)
                    print(f"  è‰ç¨¿: {p}")
        return

    # å•æ—¥æ¨¡å¼
    # æ— å‚æ•°æ—¶ï¼šå–ã€Œæœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥ã€ï¼ˆ0 ç‚¹è·‘åˆ™ä¸ºä¸Šä¸€äº¤æ˜“æ—¥ï¼Œç›˜ä¸­/æ”¶ç›˜åè·‘åˆ™ä¸ºå½“æ—¥ï¼‰
    if args.date:
        date_str = args.date
    else:
        date_str = get_recent_trading_days(1)[0]
    print(f"ğŸ“Š æ­£åœ¨é‡‡é›† {date_str} çš„å¤ç›˜æ•°æ®...\n")
    data = collect_single(date_str, use_realtime=True)
    print_report(data)
    if not args.print_only:
        _save_json(data, date_str)
        print(f"âœ… åŸå§‹æ•°æ®å·²ä¿å­˜è‡³: {DATA_DIR / f'{date_str}.json'}")
        if args.draft:
            out_path = generate_draft_review(date_str, data)
            print(f"âœ… å¤ç›˜è‰ç¨¿å·²ç”Ÿæˆ: {out_path}")


if __name__ == "__main__":
    main()
